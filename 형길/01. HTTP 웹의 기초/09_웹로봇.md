# 9장. 웹 로봇

<br>

## 소개

<br>

웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행한느 소프트웨어 프로그램이다.

웹 사이트에서 다른 웹 사이트로 떠돌아다니면서, 콘텐츠를 가져오고, 하이퍼링크를 따라가고, 그들이 발견한 데이터를 처리한다. 

이러한 종류의 로봇들은 마치 스스로 마음을 가지고 있는 것처럼 자동으로 웹 사이트들을 탐색하며, 그 방식에 따라 크롤러, 스파이더, 웜, 봇 등 다양하게 불린다.

## 9.1 크롤러와 크롤링

- 웹 크롤러는 웹 페이지를 가져오고, 그 페이지가 가리키는 모든 웹페이지를 가져오는 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다.
  
- 크롤러, 스파이더라 부르는데, HTML 하이퍼링크로 만들어진 웹을 따라 기어다니기(crawl) 때문이다.
  
- 인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다.
 
- 이 문서들은 검색 가능한 데이터베이스로 만들어져, 사용자가 특정 단어를 포함한 문서를 찾을 수 있게 해준다.

### 9.1.1 루트 집합

- 크롤러가 방문을 시작하는 URL들의 초기 집합을 루트 집합(root set)이라고 부른다.
  
- 일반적으로 웹의 대부분을 커버하기 위해 루트 집합에 너무 많은 페이지가 있을 필요는 없다.

- 일반적으로 좋은 루트 집합은 크고 인기있는 웹 사이트, 새로 생성된 페이지 목록, 자주 링크되지 않는 페이지들의 목록 등으로 구성된다.
  
- 루트 집합은 시간에 따라 성장하고, 새로운 크롤링을 위한 시드 목록이 된다.

### 9.1.2 링크 추출과 상대 링크 정상화

- 크롤러는 검색한 각 페이지 안의 URL 링크들을 파싱, 크롤링할 페이지 목록에 추가해야 한다.
  
- 이 목록은 더이상 크롤러가 방문할 새 링크가 없게 될 때까지 성장한다.

### 9.1.3 순환 피하기

<br>

<p align="center"><img src="../images/웹로봇1.png" width="80%"></p>

<br>

1. 페이지 A를 가져와, B가 A에 링크된 것을 보고 B를 가져온다.
   
2. B에서 C가 B에 링크되어있는 것을 보고 C를 가져온다.
   
3. C에서 A가 링크되는 것을 보고 A를 가져온다.

- A, B, C, A, B, C ... 를 계속 가져오게되는 순환에 빠진다.

- 로봇들은 순환을 피하기 위해서 반드시 그들이 어디를 방문했는지 알아야 한다.

### 9.1.4 루프와 중복

- 순환은 크롤러에게 해롭다.

  1. 루프에 빠져 같은 페이지를 반복해서 가져오게 되고, 네트워크 대역폭을 다 차지하면서 그 어떤 페이지도 가져올 수 없게 된다.
     
  2. 웹 서버의 부담이 늘어난다.. 크롤러의 네트워크 접근 속도가 충분히 빠르다면, 어떤 유저도 사이트에 접근할 수 없도록 막아버린다.. 이런 Denial of service는 법적 문제제기의 근거가 될 수도 있다.
     
  3. 크롤러는 많은 수의 중복된 페이지를 가져온다(dups). 크롤러 애플리케이션은 쓸모없는 중복된 컨텐츠로 채워진다. 이러한 예 중 하나가 수백 개의 똑같은 페이지를 반환하는 인터넷 검색엔진이다.

### 9.1.5 빵 부스러기의 흔적

- 전 세계 웹 콘텐츠의 상당 부분을 크롤링하려면, 복잡한 자료구조를 사용해야 한다.

- 검색 트리나 해시 테이블이 필요하다.

- 대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 유용한 기법

    1. 트리와 해시 테이블

        복잡한 로봇이면 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용한다.

    <br>

    2. 느슨한 존재 비트맵

        공간 사용을 최적화하기 위해 존재 비트 배열(presence bit array)와 같은 자료 구조를 사용한다.

        각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고, 배열 안에 대응하는 존재 비트(presence bit)를 가진다.

        Url이 크롤링 되면 해당하는 존재 비트가 만들어진다. 존재비트가 이미 존재하면 크롤러는 그 url을 이미 크롤링 했다고 간주한다.

        존재 비트 배열은 유한하므로, 해시 충돌 가능성이 존재한다. 충돌이 일어나면, 그 페이지 하나가 크롤링에서 제외되는 것이다. 큰 존재 비트 배열을 사용해 이 일을 최소화할 수 있다.

    <br>

    3. 체크포인트

        로봇 프로그램이 중단될 경우를 대비, 방문 url 목록이 디스크에 저장되었는지 확인한다.

    <br>

    4. 파티셔닝

        여러 로봇들이 동시에 일하는 농장(farm)을 활용한다.

        각 로봇엔 url들의 특정 부분이 할당되어 그에 대한 책임을 진다.

        로봇들은 url을 넘겨주거나 오동작하는 동료를 도와주거나 하는 식으로 서로 도움을 준다.

### 9.1.6 별칭(alias)과 로봇 순환

- 한 url이 또 다른 url에 대한 별칭이라면, 그 둘이 달라보여도 사실 같은 리소스이다.
  
- 다른 URL들이 같은 리소스를 가리키게 되는 몇 가지 예는 다음과 같다.
  
- 기본 포트 생략, escape character 사용, url 에 태그가 붙은 경우, 대소문자 구분의 경우, 기본 페이지 생략, 아이피주소 접근 등

### 9.1.7 URL 정규화하기

- 대부분의 웹 로봇은 url들을 표준 형식으로 정규화해, 다른 url과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려 시도한다.
  
    1. 포트 번호를 명시
    2. 이스케이핑 문자를 대응되는 문자로 변환
    3. #태그들을 제거

- 웹서버에 대한 지식 없이 중복을 피할 수 있는 좋은 방법은 없기에 해당 부분들도 고려해야 한다.
    
    1. 웹 서버가 대소문자 구분하는지
    2. 색인 페이지가 설정되었는지
    3. 가상 호스팅을 하는지
    4. 표준 형식으로 변환해도 제거할 수 없는 별칭

### 9.1.8 파일 시스템 링크 순환

- 파일 시스템의 symbolic link는 아무것도 존재하지 않으면서 끝없이 깊어지는 디렉터리를 만들 수 있으며 순환을 유발한다.

<br>

<p align="center"><img src="../images/웹로봇2.jpeg" width="80%"></p>

<br>

- http://www.foo.com/subdir/subdir/subdir/index.html 처럼 루프를 발견하지 못하면 url 길이가 로봇이나 서버의 한계를 넘을 때까지 이 순환이 계속될 것이다.

### 9.1.9 동적 가상 웹 공간

<br>

<p align="center"><img src="../images/웹로봇3.jpeg" width="80%"></p>

<br>

- 악의적인 웹마스터는 의도적으로 복잡한 크롤러 루프를 만들 수 있다.

- 서버는 새로운 가상 url을 갖고 있는 새 html 페이지를 날조하여 만들어 낸다.

- 이는 rul과 html이 매번 달라, 로봇이 순환을 감지하기 매우 어렵다.

### 9.1.10 루프와 중복 피하기

- 순환을 피하는 완벽한 방법은 없고, 잘 설계된 로봇은 순환을 피하기 위한 휴리스틱이 존재한다.

- 휴리스틱은 의심스러워 보이지만 실은 유효한 콘텐츠를 걸러버리는 손실을 유발할 수도 있다.

- 웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법들은 다음과 같다.


    1. URL 정규화

        url을 표준 형태로 변환함으로써, 중복을 제거한다.
    
    <br>

    2. 너비 우선 크롤링(breadth-first crawling)

        방문할 url을 너비 우선으로 스케줄링하여, 순환의 영향을 최소화한다.

        깊이 우선 방식으로 운용한다면 웹 사이트 하나의 순환에 빠질 수도 있다.

    <br>

    3. 스로틀링(throttling)

        웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한한다.

        순환에 빠져 사이트의 별칭에 대한 접근을 시도한다면, 접근 횟수와 중복의 총 횟수를 제한한다.

    <br>

    4. URL 크기 제한

        일정 길이를 넘는 URL의 크롤링을 거부할 수 있다.

        순환으로 인해 URL이 길어질 경우 유용하다.

        주의점은 이 기법으로 인해 가져오지 못하는 콘텐츠들도 있을 것이라는 점이다.

        요청 URL이 특정 크기에 도달할 때마다 에러 로그를 남겨 모니터링이 가능하다.

    <br>

    5. URL/사이트 블랙리스트   

        문제를 일으키는 사이트나 URL을 블랙리스트에 추가한다.
        
        크롤링을 싫어하는 사이트를 피하기 위해서도 사용된다.

    <br>

    6. 패턴 발견

        반복된 구성요소를 갖고 있는 URL을 크롤링하는 것을 거절한다.

    <br>

    7. 콘텐츠 지문(fingerprint)
   
        페이지의 콘텐츠에서 몇 바이트를 얻어내 체크섬(그 페이지 내용의 간략한 표현)을 계산한다.

        로봇이 이전에 보았던 체크섬을 가진 페이지를 가져오면, 그 페이지의 링크는 크롤링하지 않는다.

        지문 생성용으로 md5와 같은 메시지 요약 함수를 사용한다.

        웹 서버의 동적인 페이지 수정등이 방해가 될 수 있다.
    
    <br>
    
    8. 사람의 모니터링
   
        모든 상용 수준의 로봇은 사람이 쉽게 로봇의 진행 상황을 모니터링할 수 있게 진단과 로깅을 포함하도록 설계되어야 한다.

## 9.2 .로봇의 HTTP

- 로봇 또한 HTTP 명세의 규칙을 지켜야 한다.
  
- 많은 로봇이 HTTP/1.0 요청을 보내는데, 요구사항이 적기 때문이다.
  
### 9.2.1 요청 헤더 식별하기

- 로봇들은 약간의 신원 식별 헤더(user-agent header 등)를 구현하고 전송한다.

- 잘못된 크롤러의 소유자를 찾아낼 때, 서버에게 로봇이 어떤 종류의 콘텐츠를 다룰 수 있는지 정보를 주려할 때 사용된다.

- 기본적인 신원 식별 헤더는 다음과 같다.
    
    1. User-Agent - 요청을 만든 로봇의 이름.

    2. From - 사용자/관리자의 이메일 주소.

    3. Accept - 서버가 어떤 미디어 타입을 보내도 되는가. 로봇이 관심있는 유형의 콘텐츠만 받게 될 것임을 확신하는데 도움을 준다.
    
    4. Referer - 현재의 요청 URL을 포함한 문서의 URL.

### 9.2.2 가상 호스팅

- 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다. 

- Host 헤더를 포함하지 않으면 잘못된 콘텐츠를 제공할 수 있다. 이러한 이유로 HTTP/1.1은 host 헤더를 사용할 것을 요구한다.

<br>

<p align="center"><img src="../images/웹로봇4.png" width="80%"></p>

<br>

### 9.2.3 조건부 요청

- 때때로 로봇들은 극악한 양의 요청을 시도하므로, 로봇이 검색하는 콘텐츠의 양을 최소화하는 것은 의미가 있다.
  
- 인터넷 검색엔진 로봇의 경우 오직 콘텐츠가 변경되었을 때만 요청하도록 하는 것은 의미가 있다.

- 시간이나 엔터티 태그를 비교하는 조건부 HTTP 요청을 구현한다.
  
### 9.2.4 응답 다루기

- 대다수의 로봇은 단순히 GET 메서드로 콘텐츠를 요청해서 가져온다.
  
- 그러나 조건부 요청을 사용하는 로봇, 웹 탐색이나 서버와 상호작용하려는 로봇은 여러 종류의 HTTP 응답을 다룰 줄 알아야 한다.
  
    1. 상태 코드

        200 OK, 404 Not Found 등의 HTTP 상태 코드를 이해해야 한다.

        다만 모든 서버가 언제나 항상 적절한 에러 코드를 반환하지는 않는다.

    2. 엔터티
   
    메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보이다.

    콘텐츠를 다루는 서버가 제공할 수도 있는 헤더를 덮어쓰기 위한 수단이다.
        
        <meta http-equiv="Refresh" content="1; URL=index.html">
        
        해당 태그는 수신자가 문서를 마치 그 문서의 HTTP 응답 값이 Refresh 헤더를 포함한 것처럼 다루게 한다.

### 9.2.5 User-Agent 타기팅(targeting)

- 많은 웹 사이트들은 여러 기능을 지원할 수 있도록 브라우저의 종류를 감지하여 콘텐츠를 최적화한다.
  

- 사이트는 로봇에게 에러 페이지를 제공할 수 있다.(your browser does not support frames)
  
- 풍부한 기능을 갖추지 못한 브라우저나 로봇 등을 위한 유연한 페이지를 개발해야 한다.

- 사이트 관리자들은 로봇이 그들의 사이트에 방문했다가 콘텐츠를 얻을 수 없어 당황하는 일이 없도록 대비해야 한다.

## 9.3 부적절하게 동작하는 로봇들

- 폭주하는 로봇
  
    로봇은 사람보다 훨씬 빠르게 HTTP 요청을 할 수 있다.
    
    이런 로봇이 논리적인 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 극심한 부하를 안겨줄 수 있다.

    이 부하로 인해 서버는 다른 누구에게도 서비스를 못하게 된다.

    모든 로봇 저자들은 폭주 방지를 위한 보호 장치를 설계해야 한다.

- 오래된 URL
    
    로봇들의 URL 목록이 오래되었을 수 있다.
    
    존재하지 않는 문서에 대한 요청으로 에러 로그가 채워지는 등의 것, 에러 페이지를 제공하는 부하가 늘어나는 것을 좋아하지 않는 웹 사이트 관리자들을 짜증나게 한다.

- 길고 잘못된 URL

    URL이 길면 웹 서버의 처리 능력에 영향을 줄 수 있다.

- 호기심이 지나친 로봇

    로봇은 사적인 데이터에 대한 URL을 얻어 그 데이터를 인터넷 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근할 수 있도록 만들 수도 있다.

    데이터의 소유자가 사생활 침해라 여길 수도 있다.

    디렉터리의 콘텐츠를 가져오는 등의 방법으로 긁어올 때 일어날 수도 있다.

    민감한 데이터는 비밀번호, 신용카드 정보 등을 포함할 수도 있다.


- 동적 게이트웨이 접근

    로봇이 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청을 할 수도 있다.

    이 때의 데이터는 특수 목적을 위한 것일 수도 있고, 처리 비용이 많이 든다.

## 9.4 로봇 차단하기

- 1994년 로봇이 그들에게 맞지 않는 장소에 들어오지 않도록 하고, 웹마스터에게 로봇의 동작을 더 잘 제어할 수 있는 메커니즘을 제공하는 단순하고 자발적인 기법이 제안된다.
  
- Robots Exclusion Standard라 이름지어졌지만, robots.txt라 불린다.

- 웹 서버는 서버의 문서 루트에 robots.txt라 이름 붙은 선택적인 파일을 제공한다.

- 이 파일에는 어떤 로봇이 어떤 부분에 접근할 수 있는지에 대한 정보가 있다.
  
- 로봇은 웹 사이트의 어떤 다른 리소스에 접근하기 전에 우선 그 사이트의 robots.txt를 요청한다.

- 로봇은 robots.txt를 검사하여, 페이지를 요청할지 안할지를 결정한다.

## 9.4.1 로봇 차단 표준

- 임시방편으로 마련된 표준이다.

- 웹 사이트에 대한 로봇의 접근을 제어하는 능력은 불완전하지만, 없는 것보다는 낫다. 대부분의 주류 업체들과 검색엔진 크롤러들이 이 차단 표준을 지원한다.

- 3가지 버전이 있으며(0.0, 1.0, 2.0) 대부분 1.0을 표준으로 채택한다.

### 9.4.2 웹 사이트와 robots.txt 파일들

- 웹 사이트에 robots.txt파일이 존재한다면 로봇은 반드시 그 파일을 가져와 처리해야 한다.
  
- 호스트 명과 포트번호에 의해 정의되는 웹 사이트가 있다면, 그 사이트 전체에 대한 robots.txt파일은 단 하나만 존재한다.
  
- 만일 웹 사이트가 가상호스팅된다면, docroot마다 서로 다른 robots.txt가 있을 수 있다.

- robots.txt 가져오기
  
    로봇은 GET 메서드로 robots.txt를 가져온다.

    게이트웨이 애플리케이션이 robots.txt를 동적으로 생성할 수도 있을 것이다.

    robots.txt가 존재하면 서버는 text/plain 본문으로 반환한다.

    서버가 404 not found로 응답하면, 로봇은 접근을 제한하지 않는 것으로 간주한다.

    로봇은 사이트 관리자가 로봇의 접근을 추적할 수 있도록 From, User-Agent 헤더를 통해 신원을 남겨야 한다.

<br>

- 응답 코드
  
    로봇은 어떤 웹 사이트든 반드시 robots.txt를 찾아본다.

    서버가 성공(2XX)으로 응답하면, 로봇은 그 응답을 파싱하여 차단 규칙을 얻고, 사이트에서 무언가를 가져올 때 규칙에 따라야 한다.

    서버가 404로 응답하면, 로봇은 차단 규칙이 존재하지 않는다고 가정하고 robots.txt의 제약 없이 사이트에 접근할 수 있다.

    서버가 접근 제한(401, 403)으로 응답하면 로봇은 그 사이트로의 접근이 완전히 제한되어 있다고 가정해야 한다.

    요청 시도가 일시적으로 실패(503)했다면, 로봇은 그 사이트의 리소스 검색을 뒤로 미루어야 한다.
    
    리다이렉션(3xx)을 의미한다면 로봇은 리소스가 발견될 때 까지 리다이렉트를 따라가야 한다.